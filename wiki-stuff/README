#### For use with wiki-stuff scripts

#### Script to extract wiki metadata for set of ~230 dtriac articles

## Usage: extract_wiki_info.py

Objective: parse DBpedia and wikipedia articles for 2-level text (1 degree removed from PDFs)

- full text from each article without article mark-up
- categories (from Wiki ontology, listed for each article at very bottom of each page),
- anchor texts for hyperlinks to each wiki topic (how other Wiki articles link to the present article),
- outgoing links (in order to provide 3-level of related vocabulary)


Input: set of decompressed wiki files (listed below in Step #1)

Output: Two JSON files

- dtriac_docids_with_wiki_topics.json: JSON file compiling the wiki csv file;
 keys are docids, values are wiki_topics, title, and year of article

example:

{
    "329407": {
        "wiki_topics": [
            "List_of_alloys",
            "Amorphous_metal",
            "Projectile"
        ],
        "title": "",
        "year": ""
    },
    "28270": {
        "wiki_topics": [
            "Spectroscopy",
            "Fluid_mechanics",
            "Fluid_dynamics"
        ],
        "title": "",
        "year": ""
    }, ...


- wiki_topics_with_metadata.json: JSON file with all wiki stuff;
keys are wiki_topics, values are associated wiki stuff

example:

    "5-HT3_receptor": {
        "categories": [
            "Ion_channels",
            "Ionotropic_receptors",
            "Serotonin"
        ],
        "text": "5-HT3 receptor\n\nThe 5-HT receptor belongs to the Cys-loop superfamily of ligand-gated...",
        "anchor_texts": [
            "5-HT<sub>3</sub>",
            "5-HT3 receptor",
            "5-HT<sub>3</sub> receptor agonism",
            "5-HT<sub>3</sub> receptor",
            "5HT3 receptors",
            "5-hydroxytryptamine-3 (5-HT3) receptor",
            "5HT<sub>3</sub> receptor",
            "5-HT3",
            "5-HT<sub>3</sub> receptors",
            "5HT<sub>3</sub>",
            "5-hydroxytryptamine3 (5-HT3)",
            "5-HT3 agonist"
        ],
        "out_links": [
            "Ligand-gated_ion_channels",
            "5-HT",
            "Serotonin",
            "G-protein-coupled_receptor",
            "Cation",
            "Central_nervous_system",
            "Peripheral_nervous_system",
            "Nervous_systems",
            "Sodium",
            "Potassium",
            "Calcium" ... }


Steps:

Note: Links are for 2016-10 DBPedia data (later data may be available)

Storage requirements: about 300GB of storage for the decompressed wiki files

(1) Download Wikipedia XML source dump file, categories KB, nif links KB, and anchor texts KB

wget http://downloads.dbpedia.org/2016-10/core-i18n/en/pages_articles_en.xml.bz2
wget http://downloads.dbpedia.org/2016-10/core-i18n/en/article_categories_en.ttl.bz2
wget http://downloads.dbpedia.org/2016-10/core-i18n/en/nif_text_links_en.ttl.bz2
wget http://downloads.dbpedia.org/2016-10/core-i18n/en/anchor_text_en.ttl.bz2


(2) Decompress the bz2 file for the articles

bzip2 -d pages_articles_en.xml.bz2


(3) Extract the text from the articles

Documentation: http://medialab.di.unipi.it/wiki/Wikipedia_Extractor
Github: https://github.com/attardi/wikiextractor

(3.1) Get wikiextractor.py

git clone https://github.com/attardi/wikiextractor.git

(3.2) Extract text from articles into directory ../extracted/; this produces 120 directories of form: AA, AB, ...

python3 WikiExtractor.py -c -o ../extracted/ ../data/pages_articles_en.xml.bz2

(3.3) In each directory (AA, AB...), decompress all the files (wiki_00, wiki_01 ...)

I piped this through with a bash script; I do not have the exact script handy


(4) Decompress the bz2 files for categories, nif text links, and anchor text (some of these are large)

Note: add flag "-k" to keep the original compressed files

bzip2 -d -k article_categories_en.ttl.bz2
bzip2 -d -k nif_text_links_en.ttl.bz2
bzip2 -d -k anchor_text_en.ttl.bz2


(5) Download the csv file dtriac-wiki_topics from Google Drive


(6) Run extract_wiki_info.py

Sample command line:

python3 extract_wiki_info.py \
--topics_csv ../data/dtriac-wiki_topics.csv \
--categories_kb ../data/article_categories_en.ttl \
--texts_kb ~/wiki/extracted/ \
--nif_links_kb ../data/nif_text_links_en.ttl \
--anchor_texts_kb ../data/anchor_text_en.ttl



#### Script to retrieve coordinates and other metadata for a set of testing operations


## Usage: get_geocoordinates.py

Objective: parse kml files from operations associated with testing (1950s-1990s) to retrieve geocoordinates and other metadata

Input: folder of kml files from the Google drive /dtriac-19/geocoordinates.tar.gz

also need one DBPedia file to retrieve attributes from info boxes associated with each event;

**See Instructions below for links to input data**

Output: /path/to/operations_geocoordinates_metadata.json
- single JSON file with attributes for each testing operation (metadata for about 40 main operations)

example:

{
    "operation_anvil": {
        "subevents_with_geocoordinates": {
            "<name>Marsh</name>": "<coordinates>-116.02909,37.02362,0</coordinates>",
            "<name>Husky Pup</name>": "<coordinates>-116.18019,37.22174,0</coordinates>",
            "<name>Kasseri</name>": "<coordinates>-116.41244,37.29,0</coordinates>",
            "<name>Deck</name>": "<coordinates>-116.02143,37.02031,0</coordinates>",
            "<name>Inlet</name>": "<coordinates>-116.36845,37.2249,0</coordinates>",
            "<name>Leyden</name>": "<coordinates>-116.01975,37.11721,0</coordinates>",
            "<name>Chiberta</name>": "<coordinates>-116.06244,37.1276,0</coordinates>",
            "<name>Muenster</name>": "<coordinates>-116.33407,37.2965,0</coordinates>",
            "<name>Keelson</name>": "<coordinates>-116.03103,37.06921,0</coordinates>",
            "<name>Esrom</name>": "<coordinates>-116.03829,37.10655,0</coordinates>",
            "<name>Fontina</name>": "<coordinates>-116.48934,37.27136,0</coordinates>",
            "<name>Cheshire</name>": "<coordinates>-116.42113,37.242611,0</coordinates>",
            "<name>Shallows</name>": "<coordinates>-116.01649,37.02851,0</coordinates>",
            "<name>Estuary</name>": "<coordinates>-116.36549,37.30955,0</coordinates>",
            "<name>Colby</name>": "<coordinates>-116.47237,37.30594,0</coordinates>",
            "<name>Pool</name>": "<coordinates>-116.32946,37.25584,0</coordinates>",
            "<name>Strait</name>": "<coordinates>-116.05432,37.10716,0</coordinates>",
            "<name>Mighty Epic</name>": "<coordinates>-116.21333,37.20901,0</coordinates>",
            "<name>Rivoli</name>": "<coordinates>-116.06745,37.13715,0</coordinates>",
            "<name>Billet</name>": "<coordinates>-116.04456,37.07537,0</coordinates>"
        },
        "name": "Operation Anvil",
        "picture": "NTS - Underground Testing 017.jpg",
        "pictureDescription": "Anvil Husky Pup test chamber 1.",
        "country": "United States",
        "testSite": "NTS Area 12, Rainier Mesa; NTS Area 19, 20, Pahute Mesa; NTS, Areas 1-4, 6-10, Yucca Flat",
        "period": "1975",
        "numberOfTests": "21",
        "testType": "underground shaft, underground tunnel",
        "previousSeries": "Operation_Bedrock",
        "nextSeries": "Operation_Fulcrum",
        "date": "--04-24"
    },
    ...
}


Instructions:

(1) Retrieve geocoordinates.tar.gz from Google drive and decompress

tar -xvf geocoordinates.tar.gz

(1) Download and decompress infobox knowledge base

wget http://downloads.dbpedia.org/2016-10/core-i18n/en/infobox_properties_en.ttl.bz2

bzip2 -d -k infobox_properties_en.ttl.bz2

(2) Run get_geocoordinates.py

Sample command line:

python3 get_geocoordinates.py \
--geo_path ./geocoordinates/ \
--infobox_kb ../data/infobox_properties_en.ttl \
--write_path ./operations_geocoordinates_metadata.json


(3) Make visualizations from attributes (timeline of testing operations, locations, magnitude, types of tests, etc).

TODO